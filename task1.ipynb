{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b85f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "327f0c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset(dataset_folder=None, out_path=\"dblp_subset.json\"):\n",
    "    \"\"\"Read all dblp-ref-*.json files from a local `dblp-ref` folder,\n",
    "    filter them by year (2010-2015) and n_citation >= 60, and write a newline-\n",
    "    separated JSON file `dblp_subset.json` in the notebook directory.\"\"\"\n",
    "    # Try common local locations for the `dblp-ref` folder (sibling or parent)\n",
    "    candidates = [\n",
    "        os.path.join(os.getcwd(), 'dblp-ref'),\n",
    "        os.path.abspath(os.path.join(os.getcwd(), '..', 'dblp-ref')),\n",
    "        '/Users/ankushchhabra/Downloads/dblp-ref',\n",
    "    ]\n",
    "    if dataset_folder is not None:\n",
    "        candidates.insert(0, dataset_folder)\n",
    "\n",
    "    folder = None\n",
    "    for c in candidates:\n",
    "        if os.path.isdir(c):\n",
    "            folder = c\n",
    "            break\n",
    "    if folder is None:\n",
    "        raise FileNotFoundError('Could not find a local dblp-ref folder. Checked: ' + str(candidates))\n",
    "\n",
    "    print(f'Using dataset folder: {folder}')\n",
    "\n",
    "    def is_valid(paper):\n",
    "        try:\n",
    "            return 2010 <= paper.get('year', 0) <= 2015 and paper.get('n_citation', 0) >= 60\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    subset = []\n",
    "    # Process all files matching the pattern dblp-ref-*.json in the folder\n",
    "    files = sorted([f for f in os.listdir(folder) if f.startswith('dblp-ref') and f.endswith('.json')])\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f'No dblp-ref-*.json files found in {folder}')\n",
    "\n",
    "    for fname in files:\n",
    "        file_path = os.path.join(folder, fname)\n",
    "        print(f'Processing {file_path} ...')\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    paper = json.loads(line)\n",
    "                    if is_valid(paper):\n",
    "                        subset.append(paper)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "    print(f'Total papers selected: {len(subset)}')\n",
    "\n",
    "    with open(out_path, 'w', encoding='utf-8') as out:\n",
    "        for paper in subset:\n",
    "            json.dump(paper, out)\n",
    "            out.write('\\n')\n",
    "\n",
    "    print(f'Filtered dataset saved as {out_path}')\n",
    "\n",
    "# Example: extract_dataset()  # call this if you want to produce dblp_subset.json locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa53ac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dblp-ref folder: /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp-ref\n",
      "Processing /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp-ref/dblp-ref-0.json ...\n",
      "Processing /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp-ref/dblp-ref-1.json ...\n",
      "Processing /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp-ref/dblp-ref-2.json ...\n",
      "Processing /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp-ref/dblp-ref-3.json ...\n",
      "Total papers (nodes): 49572\n",
      "Total citations (edges): 163309\n",
      "Cites: ['67ee0b47-908f-4205-a35a-af3ed53cac4d', 'ea6c65c4-e688-4129-9cef-75c884099cc8']\n"
     ]
    }
   ],
   "source": [
    "def construct_graph(dataset_folder=None, subset_path=\"dblp_subset.json\"):\n",
    "    \"\"\"Construct a directed citation graph from local dblp-ref files or a\n",
    "    precomputed subset file. The function prefers a local `dblp_subset.json` if\n",
    "    present in the notebook folder; otherwise it will read the dblp-ref JSON\n",
    "    files from the discovered local folder (same search strategy used by\n",
    "    `extract_dataset`).\"\"\"\n",
    "    # locate dataset folder similarly to extract_dataset\n",
    "    candidates = [\n",
    "        os.path.join(os.getcwd(), 'dblp-ref'),\n",
    "        os.path.abspath(os.path.join(os.getcwd(), '..', 'dblp-ref')),\n",
    "        '/Users/ankushchhabra/Downloads/dblp-ref',\n",
    "    ]\n",
    "    if dataset_folder is not None:\n",
    "        candidates.insert(0, dataset_folder)\n",
    "\n",
    "    folder = None\n",
    "    for c in candidates:\n",
    "        if os.path.isdir(c):\n",
    "            folder = c\n",
    "            break\n",
    "\n",
    "    use_subset_file = os.path.isfile(subset_path)\n",
    "    if not use_subset_file and folder is None:\n",
    "        raise FileNotFoundError('No dataset found. Looked for subset file and dblp-ref folder.')\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    def is_valid(paper):\n",
    "        try:\n",
    "            return 2010 <= paper.get('year', 0) <= 2015 and paper.get('n_citation', 0) >= 60\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    all_paper_ids = set()\n",
    "    papers_source = []\n",
    "\n",
    "    if use_subset_file:\n",
    "        print(f'Using subset file: {subset_path}')\n",
    "        with open(subset_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    paper = json.loads(line.strip())\n",
    "                    papers_source.append(paper)\n",
    "                    if 'id' in paper:\n",
    "                        all_paper_ids.add(paper['id'])\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    else:\n",
    "        print(f'Using dblp-ref folder: {folder}')\n",
    "        files = sorted([f for f in os.listdir(folder) if f.startswith('dblp-ref') and f.endswith('.json')])\n",
    "        for fname in files:\n",
    "            file_path = os.path.join(folder, fname)\n",
    "            print(f'Processing {file_path} ...')\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        paper = json.loads(line.strip())\n",
    "                        if 'id' in paper and is_valid(paper):\n",
    "                            papers_source.append(paper)\n",
    "                            all_paper_ids.add(paper['id'])\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "    # Build graph from papers_source (only nodes and edges among collected ids)\n",
    "    for paper in papers_source:\n",
    "        paper_id = paper.get('id')\n",
    "        if paper_id is None:\n",
    "            continue\n",
    "        G.add_node(paper_id,\n",
    "                   title=paper.get('title'),\n",
    "                   authors=paper.get('authors'),\n",
    "                   year=paper.get('year'),\n",
    "                   venue=paper.get('venue'),\n",
    "                   n_citation=paper.get('n_citation'))\n",
    "        for ref in paper.get('references', []):\n",
    "            if ref in all_paper_ids:\n",
    "                G.add_edge(paper_id, ref)\n",
    "\n",
    "    print(f'Total papers (nodes): {G.number_of_nodes()}')\n",
    "    print(f'Total citations (edges): {G.number_of_edges()}')\n",
    "\n",
    "    # Example: list all papers cited by a specific paper (if present)\n",
    "    sample_paper = '2f9a0337-c299-496f-93c7-192cc071dbb8'\n",
    "    if sample_paper in G:\n",
    "        print('Cites:', list(G.successors(sample_paper)))\n",
    "\n",
    "    return G\n",
    "\n",
    "# Build graph using local folder / subset\n",
    "G = construct_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b6011e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Statistics\n",
      "================\n",
      "Number of vertices: 49572\n",
      "Number of edges: 163309\n",
      "Number of weakly connected components (WCC): 6985\n",
      "Number of strongly connected components (SCC): 47731\n",
      "Number of nodes in largest WCC: 41225\n",
      "Number of nodes in smallest WCC: 1\n",
      "Number of nodes in largest SCC: 171\n",
      "Number of nodes in smallest SCC: 1\n"
     ]
    }
   ],
   "source": [
    "# Basic counts\n",
    "num_vertices = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "\n",
    "# Weakly connected components (WCC)\n",
    "wccs = list(nx.weakly_connected_components(G))\n",
    "num_wcc = len(wccs)\n",
    "largest_wcc_size = max(len(c) for c in wccs) if wccs else 0\n",
    "smallest_wcc_size = min(len(c) for c in wccs) if wccs else 0\n",
    "\n",
    "# Strongly connected components (SCC)\n",
    "sccs = list(nx.strongly_connected_components(G))\n",
    "num_scc = len(sccs)\n",
    "largest_scc_size = max(len(c) for c in sccs) if sccs else 0\n",
    "smallest_scc_size = min(len(c) for c in sccs) if sccs else 0\n",
    "\n",
    "# Print results\n",
    "print(\"Graph Statistics\")\n",
    "print(\"================\")\n",
    "print(f\"Number of vertices: {num_vertices}\")\n",
    "print(f\"Number of edges: {num_edges}\")\n",
    "print(f\"Number of weakly connected components (WCC): {num_wcc}\")\n",
    "print(f\"Number of strongly connected components (SCC): {num_scc}\")\n",
    "print(f\"Number of nodes in largest WCC: {largest_wcc_size}\")\n",
    "print(f\"Number of nodes in smallest WCC: {smallest_wcc_size}\")\n",
    "print(f\"Number of nodes in largest SCC: {largest_scc_size}\")\n",
    "print(f\"Number of nodes in smallest SCC: {smallest_scc_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbae6fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset folder: /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp-ref\n",
      "Processing /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp-ref/dblp-ref-0.json ...\n",
      "Processing /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp-ref/dblp-ref-1.json ...\n",
      "Processing /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp-ref/dblp-ref-1.json ...\n",
      "Processing /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp-ref/dblp-ref-2.json ...\n",
      "Processing /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp-ref/dblp-ref-2.json ...\n",
      "Processing /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp-ref/dblp-ref-3.json ...\n",
      "Processing /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp-ref/dblp-ref-3.json ...\n",
      "Total papers selected: 49572\n",
      "Total papers selected: 49572\n",
      "Filtered dataset saved as dblp_subset.json\n",
      "Filtered dataset saved as dblp_subset.json\n"
     ]
    }
   ],
   "source": [
    "# Generate filtered subset file (writes dblp_subset.json in this folder)\n",
    "# If you have the raw dblp-ref files in a nearby folder, this will read them and create the subset.\n",
    "extract_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
