{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e787902e",
   "metadata": {},
   "source": [
    "# Task 2 — Co-citation and Bibliographic Coupling\n",
    "This notebook computes pairwise similarity between papers in `dblp_subset.json` using two measures:\n",
    "\n",
    "1. Co-citation score: number of papers that cite both paper A and paper B.\n",
    "2. Bibliographic coupling: number of references that two papers share (i.e., how many papers they both cite).\n",
    "\n",
    "The notebook reports the top-10 most similar paper pairs for each measure (showing titles and the score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91eafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c45cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/ankushchhabra/Downloads/Data Mining Assignment2/dblp_subset.json\n",
      "Read 49572 papers in 0.72s\n",
      "n = 49572\n"
     ]
    }
   ],
   "source": [
    "# Load the subset file (assumed to be in the same folder as this notebook)\n",
    "subset_path = os.path.join(os.getcwd(), 'dblp_subset.json')\n",
    "print('Reading', subset_path)\n",
    "start = time.time()\n",
    "papers = []\n",
    "with open(subset_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            papers.append(json.loads(line))\n",
    "        except Exception:\n",
    "            continue\n",
    "print(f'Read {len(papers)} papers in {time.time()-start:.2f}s')\n",
    "\n",
    "# Build mappings: id -> index, index -> title, and references (as indices)\n",
    "id_to_idx = {}\n",
    "titles = []\n",
    "refs_by_index = []\n",
    "for idx, p in enumerate(papers):\n",
    "    pid = p.get('id')\n",
    "    id_to_idx[pid] = idx\n",
    "    titles.append(p.get('title', '') or '')\n",
    "    refs_by_index.append(p.get('references', []) or [])\n",
    "\n",
    "n = len(papers)\n",
    "print('n =', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7cfdd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refs per paper: min 0 median 1 max 161\n"
     ]
    }
   ],
   "source": [
    "# Helper: convert reference id lists to indices (filter missing ids)\n",
    "refs_idx = [None] * n\n",
    "for i, refs in enumerate(refs_by_index):\n",
    "    lst = []\n",
    "    for r in refs:\n",
    "        j = id_to_idx.get(r)\n",
    "        if j is not None:\n",
    "            lst.append(j)\n",
    "    refs_idx[i] = lst\n",
    "\n",
    "# Basic stats about references per paper\n",
    "num_refs = [len(r) for r in refs_idx]\n",
    "print('refs per paper: min', min(num_refs) if num_refs else 0, 'median', sorted(num_refs)[len(num_refs)//2] if num_refs else 0, 'max', max(num_refs) if num_refs else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f251a9e",
   "metadata": {},
   "source": [
    "## Co-citation (C_ij) — number of papers that cite both i and j\n",
    "Approach: iterate over each *citing* paper k, take its reference list (indices), and increment counts for each pair (i,j) among those references. This counts how many papers co-cite i and j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6060346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-citation counts built in 0.3212602138519287 s; unique pairs = 636478\n",
      "\n",
      "Top-10 pairs by Co-citation score:\n",
      "1. (score=154)\n",
      "   Paper A: Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation\n",
      "   Paper B: ImageNet Classification with Deep Convolutional Neural Networks\n",
      "\n",
      "2. (score=148)\n",
      "   Paper A: The Pascal Visual Object Classes (VOC) Challenge\n",
      "   Paper B: Object Detection with Discriminatively Trained Part-Based Models\n",
      "\n",
      "3. (score=122)\n",
      "   Paper A: Real-time human pose recognition in parts from single depth images\n",
      "   Paper B: Real-time human pose recognition in parts from single depth images\n",
      "\n",
      "4. (score=95)\n",
      "   Paper A: Very Deep Convolutional Networks for Large-Scale Image Recognition\n",
      "   Paper B: ImageNet Classification with Deep Convolutional Neural Networks\n",
      "\n",
      "5. (score=91)\n",
      "   Paper A: DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition\n",
      "   Paper B: ImageNet Classification with Deep Convolutional Neural Networks\n",
      "\n",
      "6. (score=91)\n",
      "   Paper A: Caffe: Convolutional Architecture for Fast Feature Embedding\n",
      "   Paper B: ImageNet Classification with Deep Convolutional Neural Networks\n",
      "\n",
      "7. (score=85)\n",
      "   Paper A: OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks\n",
      "   Paper B: ImageNet Classification with Deep Convolutional Neural Networks\n",
      "\n",
      "8. (score=85)\n",
      "   Paper A: Visualizing and Understanding Convolutional Networks\n",
      "   Paper B: ImageNet Classification with Deep Convolutional Neural Networks\n",
      "\n",
      "9. (score=75)\n",
      "   Paper A: Very Deep Convolutional Networks for Large-Scale Image Recognition\n",
      "   Paper B: Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation\n",
      "\n",
      "10. (score=73)\n",
      "   Paper A: Going deeper with convolutions\n",
      "   Paper B: ImageNet Classification with Deep Convolutional Neural Networks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "co_counter = Counter()\n",
    "# For each citing paper (row), add combinations of its referenced papers\n",
    "for k, ref_list in enumerate(refs_idx):\n",
    "    if len(ref_list) < 2:\n",
    "        continue\n",
    "    # iterate over unordered pairs of references\n",
    "    for a, b in combinations(sorted(set(ref_list)), 2):\n",
    "        co_counter[(a, b)] += 1\n",
    "\n",
    "print('Co-citation counts built in', time.time()-start, 's; unique pairs =', len(co_counter))\n",
    "\n",
    "# Top-10 co-cited pairs\n",
    "top_k = 10\n",
    "print('\\nTop-{} pairs by Co-citation score:'.format(top_k))\n",
    "for rank, ((i, j), score) in enumerate(co_counter.most_common(top_k), start=1):\n",
    "    print(f'{rank}. (score={score})')\n",
    "    print('   Paper A:', titles[i])\n",
    "    print('   Paper B:', titles[j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3654fe",
   "metadata": {},
   "source": [
    "## Bibliographic coupling (B_ij) — number of shared references between papers i and j\n",
    "Approach: for each *cited* paper r, get the list of papers that cite r (i.e., the citing papers). For each pair of citing papers (i, j) in that list, increment coupling count. This counts how many common references i and j have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a0f595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliographic coupling counts built in 0.666895866394043 s; unique pairs = 1219299\n",
      "\n",
      "Top-10 pairs by Bibliographic Coupling score:\n",
      "1. (score=43)\n",
      "   Paper A: Salient Object Detection: A Benchmark\n",
      "   Paper B: Salient Object Detection: A Survey\n",
      "\n",
      "2. (score=40)\n",
      "   Paper A: Software-Defined Networking: A Comprehensive Survey\n",
      "   Paper B: Security in Software Defined Networks: A Survey\n",
      "\n",
      "3. (score=40)\n",
      "   Paper A: Software-Defined Networking: A Comprehensive Survey\n",
      "   Paper B: A Survey and a Layered Taxonomy of Software-Defined Networking\n",
      "\n",
      "4. (score=38)\n",
      "   Paper A: Design Guidelines for Spatial Modulation\n",
      "   Paper B: Spatial Modulation for Generalized MIMO: Challenges, Opportunities, and Implementation\n",
      "\n",
      "5. (score=37)\n",
      "   Paper A: Urban Computing: Concepts, Methodologies, and Applications\n",
      "   Paper B: Trajectory Data Mining: An Overview\n",
      "\n",
      "6. (score=37)\n",
      "   Paper A: Software-Defined Networking: A Comprehensive Survey\n",
      "   Paper B: A Survey on Software-Defined Networking\n",
      "\n",
      "7. (score=34)\n",
      "   Paper A: Salient Object Detection: A Survey\n",
      "   Paper B: Salient Object Detection: A Discriminative Regional Feature Integration Approach\n",
      "\n",
      "8. (score=33)\n",
      "   Paper A: A survey on information visualization: recent advances and challenges\n",
      "   Paper B: A Survey of Visual Analytics Techniques and Applications: State-of-the-Art Research and Future Challenges\n",
      "\n",
      "9. (score=32)\n",
      "   Paper A: Deep learning of representations: looking forward\n",
      "   Paper B: Representation Learning: A Review and New Perspectives\n",
      "\n",
      "10. (score=32)\n",
      "   Paper A: Deep Learning: Methods and Applications\n",
      "   Paper B: Representation Learning: A Review and New Perspectives\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Build mapping: cited index -> list of citing paper indices\n",
    "cited_to_citers = defaultdict(list)\n",
    "for citer_idx, ref_list in enumerate(refs_idx):\n",
    "    for cited in set(ref_list):\n",
    "        cited_to_citers[cited].append(citer_idx)\n",
    "\n",
    "# Now, for each cited paper, increment pairs among its citers\n",
    "bib_counter = Counter()\n",
    "for cited, citers in cited_to_citers.items():\n",
    "    if len(citers) < 2:\n",
    "        continue\n",
    "    for a, b in combinations(sorted(set(citers)), 2):\n",
    "        bib_counter[(a, b)] += 1\n",
    "\n",
    "print('Bibliographic coupling counts built in', time.time()-start, 's; unique pairs =', len(bib_counter))\n",
    "\n",
    "# Top-10 bibliographic coupling pairs\n",
    "print('\\nTop-{} pairs by Bibliographic Coupling score:'.format(top_k))\n",
    "for rank, ((i, j), score) in enumerate(bib_counter.most_common(top_k), start=1):\n",
    "    print(f'{rank}. (score={score})')\n",
    "    print('   Paper A:', titles[i])\n",
    "    print('   Paper B:', titles[j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb9b18",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- The algorithm uses combinatorial counting over reference lists and citing lists; it avoids building dense n-by-n matrices. It should be reasonably fast for the provided subset, but runtime depends on average reference list sizes.\n",
    "- If you want the results saved to CSV (pair ids, titles, score), tell me and I will add a cell that writes them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da63383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/ankushchhabra/Downloads/Data Mining Assignment2/co_top10.csv\n",
      "Wrote /Users/ankushchhabra/Downloads/Data Mining Assignment2/bib_top10.csv\n"
     ]
    }
   ],
   "source": [
    "# Save top-10 results to CSV files\n",
    "import csv\n",
    "# Build idx -> id mapping (we have id_to_idx)\n",
    "idx_to_id = [None] * n\n",
    "for pid, idx in id_to_idx.items():\n",
    "    idx_to_id[idx] = pid\n",
    "\n",
    "top_k = 10\n",
    "co_top = co_counter.most_common(top_k)\n",
    "bib_top = bib_counter.most_common(top_k)\n",
    "\n",
    "co_path = os.path.join(os.getcwd(), 'co_top10.csv')\n",
    "bib_path = os.path.join(os.getcwd(), 'bib_top10.csv')\n",
    "\n",
    "with open(co_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['rank','paperA_id','paperA_title','paperB_id','paperB_title','score'])\n",
    "    for rank, ((i,j), score) in enumerate(co_top, start=1):\n",
    "        w.writerow([rank, idx_to_id[i], titles[i], idx_to_id[j], titles[j], score])\n",
    "\n",
    "with open(bib_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['rank','paperA_id','paperA_title','paperB_id','paperB_title','score'])\n",
    "    for rank, ((i,j), score) in enumerate(bib_top, start=1):\n",
    "        w.writerow([rank, idx_to_id[i], titles[i], idx_to_id[j], titles[j], score])\n",
    "\n",
    "print('Wrote', co_path)\n",
    "print('Wrote', bib_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
